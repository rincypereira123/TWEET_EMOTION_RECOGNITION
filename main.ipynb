{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1956967341</td>\n",
       "      <td>empty</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1956967666</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1956967696</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956967789</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1956968416</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1956968477</td>\n",
       "      <td>worry</td>\n",
       "      <td>Re-pinging @ghostridah14: why didn't you go to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1956968487</td>\n",
       "      <td>sadness</td>\n",
       "      <td>I should be sleep, but im not! thinking about ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1956968636</td>\n",
       "      <td>worry</td>\n",
       "      <td>Hmmm. http://www.djhero.com/ is down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1956969035</td>\n",
       "      <td>sadness</td>\n",
       "      <td>@charviray Charlene my love. I miss you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1956969172</td>\n",
       "      <td>sadness</td>\n",
       "      <td>@kelcouch I'm sorry  at least it's Friday?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tweet_id   sentiment                                            content\n",
       "0  1956967341       empty  @tiffanylue i know  i was listenin to bad habi...\n",
       "1  1956967666     sadness  Layin n bed with a headache  ughhhh...waitin o...\n",
       "2  1956967696     sadness                Funeral ceremony...gloomy friday...\n",
       "3  1956967789  enthusiasm               wants to hang out with friends SOON!\n",
       "4  1956968416     neutral  @dannycastillo We want to trade with someone w...\n",
       "5  1956968477       worry  Re-pinging @ghostridah14: why didn't you go to...\n",
       "6  1956968487     sadness  I should be sleep, but im not! thinking about ...\n",
       "7  1956968636       worry               Hmmm. http://www.djhero.com/ is down\n",
       "8  1956969035     sadness            @charviray Charlene my love. I miss you\n",
       "9  1956969172     sadness         @kelcouch I'm sorry  at least it's Friday?"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('tweet_emotions1.csv')\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "neutral       8638\n",
       "worry         8459\n",
       "happiness     5209\n",
       "sadness       5165\n",
       "love          3842\n",
       "surprise      2187\n",
       "fun           1776\n",
       "relief        1526\n",
       "hate          1323\n",
       "empty          827\n",
       "enthusiasm     759\n",
       "boredom        179\n",
       "anger          110\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_id     0\n",
       "sentiment    0\n",
       "content      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['sentiment'].nunique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pre-Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_id(row):\n",
    "    content =' '.join(word for word in row.split() if not word.startswith('@'))\n",
    "    content = content.lower()\n",
    "    return content\n",
    "# Apply the function to the 'context' column\n",
    "data['content'] = data['content'].apply(remove_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    word = re.compile(r\"http\\S+|www\\S+\")\n",
    "    word = re.compile(r\"[.,!?#&;/]\")\n",
    "    return word.sub(\"\", text)\n",
    "\n",
    "# Apply the remove_urls function to the 'content' column\n",
    "data['content'] = data['content'].apply(remove_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_spaces(text):\n",
    "    space_pattern = re.compile(r\"\\s+\")\n",
    "    return space_pattern.sub(\" \", text).strip()\n",
    "\n",
    "data['content'] = data['content'].apply(remove_spaces)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenization , Lemmatization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\adith\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\adith\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\adith\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun1(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    lm = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lm.lemmatize(token) for token in tokens]\n",
    "    filtered_tokens = [re.sub(r'[^\\w\\s]', '', token) for token in lemmatized_tokens if token]\n",
    "    \n",
    "    return filtered_tokens\n",
    "\n",
    "data['content'] = data['content'].apply(fun1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['content'].map(len) > 0]\n",
    "data = data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1956967341</td>\n",
       "      <td>empty</td>\n",
       "      <td>[i, know, i, wa, listenin, to, bad, habit, ear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1956967666</td>\n",
       "      <td>sadness</td>\n",
       "      <td>[layin, n, bed, with, a, headache, ughhhhwaiti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1956967696</td>\n",
       "      <td>sadness</td>\n",
       "      <td>[funeral, ceremonygloomy, friday]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956967789</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>[want, to, hang, out, with, friend, soon]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1956968416</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[we, want, to, trade, with, someone, who, ha, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39909</th>\n",
       "      <td>1753918900</td>\n",
       "      <td>happiness</td>\n",
       "      <td>[succesfully, following, tayla]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39910</th>\n",
       "      <td>1753919001</td>\n",
       "      <td>love</td>\n",
       "      <td>[happy, mother, day, all, my, love]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39911</th>\n",
       "      <td>1753919005</td>\n",
       "      <td>love</td>\n",
       "      <td>[happy, mother, s, day, to, all, the, mommy, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39912</th>\n",
       "      <td>1753919043</td>\n",
       "      <td>happiness</td>\n",
       "      <td>[wassup, beautiful, follow, me, peep, out, my,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39913</th>\n",
       "      <td>1753919049</td>\n",
       "      <td>love</td>\n",
       "      <td>[bullet, train, from, tokyo, the, gf, and, i, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39914 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id   sentiment  \\\n",
       "0      1956967341       empty   \n",
       "1      1956967666     sadness   \n",
       "2      1956967696     sadness   \n",
       "3      1956967789  enthusiasm   \n",
       "4      1956968416     neutral   \n",
       "...           ...         ...   \n",
       "39909  1753918900   happiness   \n",
       "39910  1753919001        love   \n",
       "39911  1753919005        love   \n",
       "39912  1753919043   happiness   \n",
       "39913  1753919049        love   \n",
       "\n",
       "                                                 content  \n",
       "0      [i, know, i, wa, listenin, to, bad, habit, ear...  \n",
       "1      [layin, n, bed, with, a, headache, ughhhhwaiti...  \n",
       "2                      [funeral, ceremonygloomy, friday]  \n",
       "3              [want, to, hang, out, with, friend, soon]  \n",
       "4      [we, want, to, trade, with, someone, who, ha, ...  \n",
       "...                                                  ...  \n",
       "39909                    [succesfully, following, tayla]  \n",
       "39910                [happy, mother, day, all, my, love]  \n",
       "39911  [happy, mother, s, day, to, all, the, mommy, o...  \n",
       "39912  [wassup, beautiful, follow, me, peep, out, my,...  \n",
       "39913  [bullet, train, from, tokyo, the, gf, and, i, ...  \n",
       "\n",
       "[39914 rows x 3 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finding out the Max Length Value**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_lengths = []\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in data.iterrows():\n",
    "    tokens = row['content']\n",
    "    token_lengths.append(len(tokens))\n",
    "\n",
    "# Convert the list of token lengths to a DataFrame column\n",
    "data['token_length'] = token_lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.0\n"
     ]
    }
   ],
   "source": [
    "list1 = []\n",
    "for x in data['token_length']:\n",
    "    list1.append(x)\n",
    "percentile_95 = np.percentile(token_lengths, 95)\n",
    "print(percentile_95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.32735381069299\n"
     ]
    }
   ],
   "source": [
    "import statistics as st\n",
    "print(st.mean(list1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n"
     ]
    }
   ],
   "source": [
    "print(max(list1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data.sample(n = 15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "      <th>token_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6829</th>\n",
       "      <td>1961463950</td>\n",
       "      <td>worry</td>\n",
       "      <td>[, o, awww, leave, off, shes, ace, shes, one, ...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39703</th>\n",
       "      <td>1753871880</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[lmao, , that, would, have, taken, me, at, lea...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30834</th>\n",
       "      <td>1751751523</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[it, 1111make, a, wish]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27281</th>\n",
       "      <td>1695731147</td>\n",
       "      <td>happiness</td>\n",
       "      <td>[happy, star, war, day, may, the, 4th, be, wit...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12872</th>\n",
       "      <td>1963781502</td>\n",
       "      <td>worry</td>\n",
       "      <td>[aww, thats, too, bad, you, lost, it, though]</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3725</th>\n",
       "      <td>1958113021</td>\n",
       "      <td>worry</td>\n",
       "      <td>[bahhhh, cold, weather, is, making, my, teeth,...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32262</th>\n",
       "      <td>1752257523</td>\n",
       "      <td>sadness</td>\n",
       "      <td>[ducked, off, i, ll, be, back, in, ga, tomorrow]</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26325</th>\n",
       "      <td>1695376673</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[currently, uploading, wordpress, this, ll, gi...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39324</th>\n",
       "      <td>1753819653</td>\n",
       "      <td>love</td>\n",
       "      <td>[happy, mother, s, day, mom, you, are, wonderf...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39806</th>\n",
       "      <td>1753902379</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[cook, do, you, wan, na, measure, my, dick, it...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id  sentiment  \\\n",
       "6829   1961463950      worry   \n",
       "39703  1753871880    neutral   \n",
       "30834  1751751523    neutral   \n",
       "27281  1695731147  happiness   \n",
       "12872  1963781502      worry   \n",
       "...           ...        ...   \n",
       "3725   1958113021      worry   \n",
       "32262  1752257523    sadness   \n",
       "26325  1695376673    neutral   \n",
       "39324  1753819653       love   \n",
       "39806  1753902379    neutral   \n",
       "\n",
       "                                                 content  token_length  \n",
       "6829   [, o, awww, leave, off, shes, ace, shes, one, ...            13  \n",
       "39703  [lmao, , that, would, have, taken, me, at, lea...            11  \n",
       "30834                            [it, 1111make, a, wish]             4  \n",
       "27281  [happy, star, war, day, may, the, 4th, be, wit...            10  \n",
       "12872      [aww, thats, too, bad, you, lost, it, though]             8  \n",
       "...                                                  ...           ...  \n",
       "3725   [bahhhh, cold, weather, is, making, my, teeth,...             8  \n",
       "32262   [ducked, off, i, ll, be, back, in, ga, tomorrow]             9  \n",
       "26325  [currently, uploading, wordpress, this, ll, gi...            16  \n",
       "39324  [happy, mother, s, day, mom, you, are, wonderf...            12  \n",
       "39806  [cook, do, you, wan, na, measure, my, dick, it...            10  \n",
       "\n",
       "[15000 rows x 4 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Model,GPT2Tokenizer\n",
    "import torch\n",
    "model = GPT2Model.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Initialize the GPT-2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Set the maximum sequence length for padding\n",
    "max_length = 30\n",
    "\n",
    "# Create a new column 'features' in the DataFrame\n",
    "data1['features'] = None\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in data1.iterrows():\n",
    "    tokens = row['content']\n",
    "\n",
    "    # Truncate or pad the tokens to the maximum length\n",
    "    tokens = tokens[:max_length] if len(tokens) > max_length else tokens + ['[PAD]'] * (max_length - len(tokens))\n",
    "\n",
    "    # Convert tokens into token IDs\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    # Skip empty tokens\n",
    "    if len(input_ids) == 0:\n",
    "        continue\n",
    "\n",
    "    # Convert token IDs into input tensors\n",
    "    input_ids = torch.tensor(input_ids).unsqueeze(0)\n",
    "\n",
    "    # Pass the non-empty input tensors through the pre-trained GPT-2 model to extract the features\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "\n",
    "    features = outputs.last_hidden_state\n",
    "\n",
    "    # Store the features in the 'features' column for the current row\n",
    "    data1.at[index, 'features'] = features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data1.dropna(subset=['features'])\n",
    "\n",
    "# Convert the 'features' column to a NumPy array\n",
    "x = np.stack([tensor.numpy() for tensor in data1['features'].values])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Target Encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(data1['sentiment'])\n",
    "label_mapping = {index: label for index, label in enumerate(encoder.classes_)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'anger',\n",
       " 1: 'boredom',\n",
       " 2: 'empty',\n",
       " 3: 'enthusiasm',\n",
       " 4: 'fun',\n",
       " 5: 'happiness',\n",
       " 6: 'hate',\n",
       " 7: 'love',\n",
       " 8: 'neutral',\n",
       " 9: 'relief',\n",
       " 10: 'sadness',\n",
       " 11: 'surprise',\n",
       " 12: 'worry'}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "y = to_categorical(y, num_classes=13)\n",
    "y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 1, 30, 768)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(15000, 13)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x.shape)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.reshape(x,(15000,30,768))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 30, 768)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(15000, 13)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x.shape)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size= 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(30, 768), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(13, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "375/375 [==============================] - 71s 181ms/step - loss: 2.2167 - accuracy: 0.2065 - val_loss: 2.1522 - val_accuracy: 0.2067\n",
      "Epoch 2/100\n",
      "375/375 [==============================] - 70s 187ms/step - loss: 2.1648 - accuracy: 0.2218 - val_loss: 2.1406 - val_accuracy: 0.2520\n",
      "Epoch 3/100\n",
      "375/375 [==============================] - 71s 189ms/step - loss: 2.1519 - accuracy: 0.2470 - val_loss: 2.1324 - val_accuracy: 0.2487\n",
      "Epoch 4/100\n",
      "375/375 [==============================] - 71s 191ms/step - loss: 2.1299 - accuracy: 0.2520 - val_loss: 2.0921 - val_accuracy: 0.2697\n",
      "Epoch 5/100\n",
      "375/375 [==============================] - 71s 189ms/step - loss: 2.0971 - accuracy: 0.2673 - val_loss: 2.0799 - val_accuracy: 0.2727\n",
      "Epoch 6/100\n",
      "375/375 [==============================] - 71s 188ms/step - loss: 2.0831 - accuracy: 0.2757 - val_loss: 2.0746 - val_accuracy: 0.2793\n",
      "Epoch 7/100\n",
      "375/375 [==============================] - 70s 187ms/step - loss: 2.0697 - accuracy: 0.2793 - val_loss: 2.0691 - val_accuracy: 0.2870\n",
      "Epoch 8/100\n",
      "375/375 [==============================] - 70s 188ms/step - loss: 2.0625 - accuracy: 0.2890 - val_loss: 2.0515 - val_accuracy: 0.2910\n",
      "Epoch 9/100\n",
      "375/375 [==============================] - 71s 188ms/step - loss: 2.0501 - accuracy: 0.2951 - val_loss: 2.0537 - val_accuracy: 0.2907\n",
      "Epoch 10/100\n",
      "375/375 [==============================] - 71s 190ms/step - loss: 2.0398 - accuracy: 0.3021 - val_loss: 2.0457 - val_accuracy: 0.2980\n",
      "Epoch 11/100\n",
      "375/375 [==============================] - 71s 190ms/step - loss: 2.0314 - accuracy: 0.3017 - val_loss: 2.0369 - val_accuracy: 0.3017\n",
      "Epoch 12/100\n",
      "375/375 [==============================] - 71s 190ms/step - loss: 2.0218 - accuracy: 0.3092 - val_loss: 2.0430 - val_accuracy: 0.2987\n",
      "Epoch 13/100\n",
      "375/375 [==============================] - 72s 192ms/step - loss: 2.0124 - accuracy: 0.3124 - val_loss: 2.0215 - val_accuracy: 0.3070\n",
      "Epoch 14/100\n",
      "375/375 [==============================] - 73s 195ms/step - loss: 2.0061 - accuracy: 0.3174 - val_loss: 2.0411 - val_accuracy: 0.3077\n",
      "Epoch 15/100\n",
      "375/375 [==============================] - 70s 188ms/step - loss: 2.0037 - accuracy: 0.3175 - val_loss: 2.0420 - val_accuracy: 0.2897\n",
      "Epoch 16/100\n",
      "375/375 [==============================] - 70s 187ms/step - loss: 1.9901 - accuracy: 0.3277 - val_loss: 2.0068 - val_accuracy: 0.3097\n",
      "Epoch 17/100\n",
      "375/375 [==============================] - 70s 186ms/step - loss: 1.9800 - accuracy: 0.3271 - val_loss: 2.0115 - val_accuracy: 0.3083\n",
      "Epoch 18/100\n",
      "375/375 [==============================] - 71s 189ms/step - loss: 1.9720 - accuracy: 0.3292 - val_loss: 2.0137 - val_accuracy: 0.3140\n",
      "Epoch 19/100\n",
      "375/375 [==============================] - 70s 187ms/step - loss: 1.9648 - accuracy: 0.3322 - val_loss: 2.0160 - val_accuracy: 0.3107\n",
      "Epoch 20/100\n",
      "375/375 [==============================] - 71s 189ms/step - loss: 1.9614 - accuracy: 0.3368 - val_loss: 2.0065 - val_accuracy: 0.3137\n",
      "Epoch 21/100\n",
      "375/375 [==============================] - 71s 189ms/step - loss: 1.9568 - accuracy: 0.3413 - val_loss: 2.0080 - val_accuracy: 0.3200\n",
      "Epoch 22/100\n",
      "375/375 [==============================] - 71s 188ms/step - loss: 1.9414 - accuracy: 0.3472 - val_loss: 2.0265 - val_accuracy: 0.3163\n",
      "Epoch 23/100\n",
      "375/375 [==============================] - 71s 189ms/step - loss: 1.9289 - accuracy: 0.3487 - val_loss: 2.0254 - val_accuracy: 0.3117\n",
      "Epoch 24/100\n",
      "375/375 [==============================] - 72s 191ms/step - loss: 1.9287 - accuracy: 0.3494 - val_loss: 2.0399 - val_accuracy: 0.3120\n",
      "Epoch 25/100\n",
      "375/375 [==============================] - 72s 191ms/step - loss: 1.9159 - accuracy: 0.3530 - val_loss: 2.0264 - val_accuracy: 0.3133\n",
      "Epoch 26/100\n",
      "375/375 [==============================] - 71s 191ms/step - loss: 1.9151 - accuracy: 0.3530 - val_loss: 2.0269 - val_accuracy: 0.3140\n",
      "Epoch 27/100\n",
      "375/375 [==============================] - 71s 190ms/step - loss: 1.8977 - accuracy: 0.3601 - val_loss: 2.0379 - val_accuracy: 0.3067\n",
      "Epoch 28/100\n",
      "375/375 [==============================] - 71s 190ms/step - loss: 1.8892 - accuracy: 0.3635 - val_loss: 2.0270 - val_accuracy: 0.3160\n",
      "Epoch 29/100\n",
      "375/375 [==============================] - 72s 191ms/step - loss: 1.8813 - accuracy: 0.3617 - val_loss: 2.0418 - val_accuracy: 0.3190\n",
      "Epoch 30/100\n",
      "375/375 [==============================] - 72s 191ms/step - loss: 1.8701 - accuracy: 0.3737 - val_loss: 2.0486 - val_accuracy: 0.3173\n",
      "Epoch 31/100\n",
      "375/375 [==============================] - 71s 191ms/step - loss: 1.8667 - accuracy: 0.3702 - val_loss: 2.0285 - val_accuracy: 0.3217\n",
      "Epoch 32/100\n",
      "375/375 [==============================] - 71s 190ms/step - loss: 1.8502 - accuracy: 0.3819 - val_loss: 2.0303 - val_accuracy: 0.3187\n",
      "Epoch 33/100\n",
      "375/375 [==============================] - 71s 190ms/step - loss: 1.8386 - accuracy: 0.3828 - val_loss: 2.0603 - val_accuracy: 0.3200\n",
      "Epoch 34/100\n",
      "375/375 [==============================] - 72s 191ms/step - loss: 1.8388 - accuracy: 0.3826 - val_loss: 2.0523 - val_accuracy: 0.3120\n",
      "Epoch 35/100\n",
      "375/375 [==============================] - 71s 190ms/step - loss: 1.8256 - accuracy: 0.3812 - val_loss: 2.0863 - val_accuracy: 0.3140\n",
      "Epoch 36/100\n",
      "375/375 [==============================] - 71s 190ms/step - loss: 1.8173 - accuracy: 0.3885 - val_loss: 2.0583 - val_accuracy: 0.3167\n",
      "Epoch 37/100\n",
      "375/375 [==============================] - 71s 190ms/step - loss: 1.8123 - accuracy: 0.3882 - val_loss: 2.0532 - val_accuracy: 0.3160\n",
      "Epoch 38/100\n",
      "375/375 [==============================] - 72s 191ms/step - loss: 1.7869 - accuracy: 0.3978 - val_loss: 2.1304 - val_accuracy: 0.3170\n",
      "Epoch 39/100\n",
      "375/375 [==============================] - 76s 202ms/step - loss: 1.7877 - accuracy: 0.3986 - val_loss: 2.0968 - val_accuracy: 0.3097\n",
      "Epoch 40/100\n",
      "375/375 [==============================] - 76s 204ms/step - loss: 1.7784 - accuracy: 0.3993 - val_loss: 2.1192 - val_accuracy: 0.3013\n",
      "Epoch 41/100\n",
      "375/375 [==============================] - 71s 189ms/step - loss: 1.7634 - accuracy: 0.4014 - val_loss: 2.1100 - val_accuracy: 0.3073\n",
      "Epoch 42/100\n",
      "375/375 [==============================] - 71s 189ms/step - loss: 1.7523 - accuracy: 0.4112 - val_loss: 2.1942 - val_accuracy: 0.3133\n",
      "Epoch 43/100\n",
      "375/375 [==============================] - 71s 190ms/step - loss: 1.7413 - accuracy: 0.4117 - val_loss: 2.1474 - val_accuracy: 0.3097\n",
      "Epoch 44/100\n",
      "375/375 [==============================] - 71s 189ms/step - loss: 1.7294 - accuracy: 0.4180 - val_loss: 2.1867 - val_accuracy: 0.3143\n",
      "Epoch 45/100\n",
      "375/375 [==============================] - 71s 189ms/step - loss: 1.7138 - accuracy: 0.4224 - val_loss: 2.1778 - val_accuracy: 0.2987\n",
      "Epoch 46/100\n",
      "375/375 [==============================] - 71s 188ms/step - loss: 1.7219 - accuracy: 0.4129 - val_loss: 2.1492 - val_accuracy: 0.2967\n",
      "Epoch 47/100\n",
      "375/375 [==============================] - 71s 189ms/step - loss: 1.7006 - accuracy: 0.4268 - val_loss: 2.2099 - val_accuracy: 0.3100\n",
      "Epoch 48/100\n",
      "375/375 [==============================] - 71s 189ms/step - loss: 1.6853 - accuracy: 0.4288 - val_loss: 2.1767 - val_accuracy: 0.3053\n",
      "Epoch 49/100\n",
      "375/375 [==============================] - 70s 188ms/step - loss: 1.6837 - accuracy: 0.4320 - val_loss: 2.2602 - val_accuracy: 0.3050\n",
      "Epoch 50/100\n",
      "375/375 [==============================] - 72s 192ms/step - loss: 1.6605 - accuracy: 0.4378 - val_loss: 2.1878 - val_accuracy: 0.3043\n",
      "Epoch 51/100\n",
      "375/375 [==============================] - 74s 198ms/step - loss: 1.6537 - accuracy: 0.4366 - val_loss: 2.2154 - val_accuracy: 0.3033\n",
      "Epoch 52/100\n",
      "375/375 [==============================] - 79s 210ms/step - loss: 1.6520 - accuracy: 0.4412 - val_loss: 2.2307 - val_accuracy: 0.3027\n",
      "Epoch 53/100\n",
      "375/375 [==============================] - 75s 201ms/step - loss: 1.6421 - accuracy: 0.4431 - val_loss: 2.2689 - val_accuracy: 0.2963\n",
      "Epoch 54/100\n",
      "375/375 [==============================] - 74s 198ms/step - loss: 1.6293 - accuracy: 0.4452 - val_loss: 2.2644 - val_accuracy: 0.3087\n",
      "Epoch 55/100\n",
      "375/375 [==============================] - 78s 207ms/step - loss: 1.6106 - accuracy: 0.4539 - val_loss: 2.2603 - val_accuracy: 0.3110\n",
      "Epoch 56/100\n",
      "375/375 [==============================] - 73s 196ms/step - loss: 1.6130 - accuracy: 0.4543 - val_loss: 2.3879 - val_accuracy: 0.2823\n",
      "Epoch 57/100\n",
      "375/375 [==============================] - 74s 198ms/step - loss: 1.5896 - accuracy: 0.4580 - val_loss: 2.3585 - val_accuracy: 0.2927\n",
      "Epoch 58/100\n",
      "375/375 [==============================] - 72s 192ms/step - loss: 1.5735 - accuracy: 0.4630 - val_loss: 2.3459 - val_accuracy: 0.3013\n",
      "Epoch 59/100\n",
      "375/375 [==============================] - 75s 200ms/step - loss: 1.5709 - accuracy: 0.4652 - val_loss: 2.4581 - val_accuracy: 0.2967\n",
      "Epoch 60/100\n",
      "375/375 [==============================] - 77s 204ms/step - loss: 1.5571 - accuracy: 0.4736 - val_loss: 2.3810 - val_accuracy: 0.2957\n",
      "Epoch 61/100\n",
      "375/375 [==============================] - 80s 212ms/step - loss: 1.5591 - accuracy: 0.4716 - val_loss: 2.3120 - val_accuracy: 0.3003\n",
      "Epoch 62/100\n",
      "375/375 [==============================] - 79s 212ms/step - loss: 1.5296 - accuracy: 0.4829 - val_loss: 2.4322 - val_accuracy: 0.3030\n",
      "Epoch 63/100\n",
      "375/375 [==============================] - 82s 219ms/step - loss: 1.5337 - accuracy: 0.4808 - val_loss: 2.3996 - val_accuracy: 0.3013\n",
      "Epoch 64/100\n",
      "375/375 [==============================] - 87s 232ms/step - loss: 1.5224 - accuracy: 0.4825 - val_loss: 2.4509 - val_accuracy: 0.2990\n",
      "Epoch 65/100\n",
      "375/375 [==============================] - 73s 194ms/step - loss: 1.5109 - accuracy: 0.4857 - val_loss: 2.4244 - val_accuracy: 0.3063\n",
      "Epoch 66/100\n",
      "375/375 [==============================] - 71s 189ms/step - loss: 1.5011 - accuracy: 0.4904 - val_loss: 2.4348 - val_accuracy: 0.2943\n",
      "Epoch 67/100\n",
      "375/375 [==============================] - 75s 201ms/step - loss: 1.4934 - accuracy: 0.4908 - val_loss: 2.5544 - val_accuracy: 0.3003\n",
      "Epoch 68/100\n",
      "375/375 [==============================] - 73s 195ms/step - loss: 1.4845 - accuracy: 0.4952 - val_loss: 2.5496 - val_accuracy: 0.2953\n",
      "Epoch 69/100\n",
      "375/375 [==============================] - 71s 190ms/step - loss: 1.4639 - accuracy: 0.5027 - val_loss: 2.4481 - val_accuracy: 0.2907\n",
      "Epoch 70/100\n",
      "375/375 [==============================] - 76s 202ms/step - loss: 1.4748 - accuracy: 0.5012 - val_loss: 2.4994 - val_accuracy: 0.2843\n",
      "Epoch 71/100\n",
      "375/375 [==============================] - 77s 205ms/step - loss: 1.4603 - accuracy: 0.4996 - val_loss: 2.5406 - val_accuracy: 0.2903\n",
      "Epoch 72/100\n",
      "375/375 [==============================] - 73s 195ms/step - loss: 1.4511 - accuracy: 0.5040 - val_loss: 2.6795 - val_accuracy: 0.2850\n",
      "Epoch 73/100\n",
      "375/375 [==============================] - 69s 183ms/step - loss: 1.4297 - accuracy: 0.5106 - val_loss: 2.6143 - val_accuracy: 0.2820\n",
      "Epoch 74/100\n",
      "375/375 [==============================] - 69s 184ms/step - loss: 1.4306 - accuracy: 0.5111 - val_loss: 2.6807 - val_accuracy: 0.2913\n",
      "Epoch 75/100\n",
      "375/375 [==============================] - 72s 192ms/step - loss: 1.4247 - accuracy: 0.5146 - val_loss: 2.5562 - val_accuracy: 0.2833\n",
      "Epoch 76/100\n",
      "375/375 [==============================] - 82s 218ms/step - loss: 1.4133 - accuracy: 0.5235 - val_loss: 2.5908 - val_accuracy: 0.2850\n",
      "Epoch 77/100\n",
      "375/375 [==============================] - 78s 208ms/step - loss: 1.3968 - accuracy: 0.5242 - val_loss: 2.7872 - val_accuracy: 0.2843\n",
      "Epoch 78/100\n",
      "375/375 [==============================] - 75s 200ms/step - loss: 1.3906 - accuracy: 0.5252 - val_loss: 2.6674 - val_accuracy: 0.2863\n",
      "Epoch 79/100\n",
      "375/375 [==============================] - 75s 200ms/step - loss: 1.3808 - accuracy: 0.5315 - val_loss: 2.7878 - val_accuracy: 0.2850\n",
      "Epoch 80/100\n",
      "375/375 [==============================] - 93s 247ms/step - loss: 1.3857 - accuracy: 0.5253 - val_loss: 2.7772 - val_accuracy: 0.2840\n",
      "Epoch 81/100\n",
      "375/375 [==============================] - 118s 315ms/step - loss: 1.3764 - accuracy: 0.5293 - val_loss: 2.7500 - val_accuracy: 0.2733\n",
      "Epoch 82/100\n",
      "375/375 [==============================] - 116s 309ms/step - loss: 1.3560 - accuracy: 0.5340 - val_loss: 2.7157 - val_accuracy: 0.2903\n",
      "Epoch 83/100\n",
      "375/375 [==============================] - 87s 232ms/step - loss: 1.3510 - accuracy: 0.5379 - val_loss: 2.8458 - val_accuracy: 0.2840\n",
      "Epoch 84/100\n",
      "375/375 [==============================] - 79s 210ms/step - loss: 1.3442 - accuracy: 0.5398 - val_loss: 2.8467 - val_accuracy: 0.2797\n",
      "Epoch 85/100\n",
      "375/375 [==============================] - 76s 204ms/step - loss: 1.3313 - accuracy: 0.5425 - val_loss: 2.8376 - val_accuracy: 0.2820\n",
      "Epoch 86/100\n",
      "375/375 [==============================] - 97s 258ms/step - loss: 1.3441 - accuracy: 0.5340 - val_loss: 2.8583 - val_accuracy: 0.2897\n",
      "Epoch 87/100\n",
      "375/375 [==============================] - 81s 217ms/step - loss: 1.3203 - accuracy: 0.5454 - val_loss: 2.8861 - val_accuracy: 0.2780\n",
      "Epoch 88/100\n",
      "375/375 [==============================] - 77s 206ms/step - loss: 1.3062 - accuracy: 0.5512 - val_loss: 2.8329 - val_accuracy: 0.2870\n",
      "Epoch 89/100\n",
      "375/375 [==============================] - 79s 211ms/step - loss: 1.3020 - accuracy: 0.5548 - val_loss: 2.9287 - val_accuracy: 0.2753\n",
      "Epoch 90/100\n",
      "375/375 [==============================] - 76s 202ms/step - loss: 1.2925 - accuracy: 0.5558 - val_loss: 2.8973 - val_accuracy: 0.2770\n",
      "Epoch 91/100\n",
      "375/375 [==============================] - 79s 211ms/step - loss: 1.2751 - accuracy: 0.5646 - val_loss: 2.9848 - val_accuracy: 0.2850\n",
      "Epoch 92/100\n",
      "375/375 [==============================] - 78s 208ms/step - loss: 1.2975 - accuracy: 0.5533 - val_loss: 2.8354 - val_accuracy: 0.2833\n",
      "Epoch 93/100\n",
      "375/375 [==============================] - 89s 238ms/step - loss: 1.2898 - accuracy: 0.5562 - val_loss: 3.0075 - val_accuracy: 0.2733\n",
      "Epoch 94/100\n",
      "375/375 [==============================] - 100s 267ms/step - loss: 1.2705 - accuracy: 0.5637 - val_loss: 2.8617 - val_accuracy: 0.2900\n",
      "Epoch 95/100\n",
      "375/375 [==============================] - 67s 179ms/step - loss: 1.2672 - accuracy: 0.5606 - val_loss: 2.9378 - val_accuracy: 0.2743\n",
      "Epoch 96/100\n",
      "375/375 [==============================] - 61s 164ms/step - loss: 1.2537 - accuracy: 0.5694 - val_loss: 3.0914 - val_accuracy: 0.2783\n",
      "Epoch 97/100\n",
      "375/375 [==============================] - 58s 156ms/step - loss: 1.2612 - accuracy: 0.5629 - val_loss: 2.9537 - val_accuracy: 0.2760\n",
      "Epoch 98/100\n",
      "375/375 [==============================] - 58s 154ms/step - loss: 1.2400 - accuracy: 0.5721 - val_loss: 3.1226 - val_accuracy: 0.2800\n",
      "Epoch 99/100\n",
      "375/375 [==============================] - 58s 153ms/step - loss: 1.2383 - accuracy: 0.5751 - val_loss: 3.0419 - val_accuracy: 0.2683\n",
      "Epoch 100/100\n",
      "375/375 [==============================] - 68s 181ms/step - loss: 1.2320 - accuracy: 0.5747 - val_loss: 3.1143 - val_accuracy: 0.2693\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e01d84b4d0>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=32, epochs=100, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 8s 87ms/step - loss: 3.1143 - accuracy: 0.2693\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2693333327770233"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PreTrained** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data1.sample(n = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data['features']\n",
    "x = np.stack([tensor.numpy() for tensor in data1['features'].values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(data['sentiment'])\n",
    "label_mapping = {index: label for index, label in enumerate(encoder.classes_)}\n",
    "from keras.utils import to_categorical\n",
    "y = to_categorical(y, num_classes=13)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 212336640000 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[114], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m X_tensor_test \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(x_test, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong)\n\u001b[0;32m     25\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m---> 26\u001b[0m     logits_test \u001b[39m=\u001b[39m classification_model(X_tensor_test)\n\u001b[0;32m     28\u001b[0m probs_test \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39msoftmax(logits_test, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     29\u001b[0m predicted_classes_test \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(probs_test, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\adith\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[114], line 14\u001b[0m, in \u001b[0;36mGPT2ClassificationModel.forward\u001b[1;34m(self, input_ids)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, input_ids):\n\u001b[1;32m---> 14\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgpt2(input_ids)[\u001b[39m0\u001b[39m]\n\u001b[0;32m     15\u001b[0m     pooled_output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(outputs, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     16\u001b[0m     pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[1;32mc:\\Users\\adith\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\adith\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:846\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    843\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mn_layer)\n\u001b[0;32m    845\u001b[0m \u001b[39mif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 846\u001b[0m     inputs_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwte(input_ids)\n\u001b[0;32m    847\u001b[0m position_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwpe(position_ids)\n\u001b[0;32m    848\u001b[0m hidden_states \u001b[39m=\u001b[39m inputs_embeds \u001b[39m+\u001b[39m position_embeds\n",
      "File \u001b[1;32mc:\\Users\\adith\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\adith\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 162\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[0;32m    163\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[0;32m    164\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[1;32mc:\\Users\\adith\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2204\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2205\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2206\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2207\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2208\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2210\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 212336640000 bytes."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Model\n",
    "\n",
    "class GPT2ClassificationModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(GPT2ClassificationModel, self).__init__()\n",
    "        self.gpt2 = GPT2Model.from_pretrained('gpt2')\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(768, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        outputs = self.gpt2(input_ids)[0]\n",
    "        pooled_output = torch.mean(outputs, dim=1)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.fc(pooled_output)\n",
    "        return logits\n",
    "\n",
    "classification_model = GPT2ClassificationModel(13)\n",
    "# Convert the model to evaluation mode\n",
    "classification_model.eval()\n",
    "X_tensor_test = torch.tensor(x_test, dtype=torch.long)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits_test = classification_model(X_tensor_test)\n",
    "\n",
    "probs_test = nn.functional.softmax(logits_test, dim=1)\n",
    "predicted_classes_test = torch.argmax(probs_test, dim=1)\n",
    "\n",
    "# Convert predicted classes to numpy array\n",
    "y_test_pred = predicted_classes_test.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test,y_test_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
